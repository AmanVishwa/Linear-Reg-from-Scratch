{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8e7025c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ec5b8a",
   "metadata": {},
   "source": [
    "### 1. Write a function to generate a data matrix X. Inputs: Number of samples, feature dimension. Output: Data matrix X.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5f4fc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to calculate data matrix X\n",
    "def data_matrix(samples,dim):\n",
    "    #for returning random output in every execution according to number of sample and feature dimension\n",
    "    return np.random.normal(size=(samples, dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8eac485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test\n",
    "# x = data_matrix(100,5)\n",
    "# x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2f4815",
   "metadata": {},
   "source": [
    "### 2. Write a function to generate dependent variable column t.                                                                                \n",
    "a) Inputs: Data matrix X, weight vector for each column, bias w0, noise variance\n",
    "\n",
    "b) Output: Target vector t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f54761e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function uses the numpy.random.normal function to generate random numbers with a normal distribution. \n",
    "# The loc parameter specifies the mean of the distribution, which is set to 0. \n",
    "# The scale parameter specifies the standard deviation of the distribution,\n",
    "# which is set to the value of the std_dev input argument.\n",
    "# The size parameter specifies the number of random numbers to generate.\n",
    "#generates an array of random numbers that follows a normal distribution with a mean of 0 and a standard deviation of \"noise_variance\"\n",
    "def noise_def(std_dev,s):\n",
    "    return np.random.normal(0, scale=std_dev, size=s)\n",
    "\n",
    "#As we have to generate the dependent variable t\n",
    "#Acoording to {t = (x*w) + w0 + noise}\n",
    "def depend_t(X, weight, w0, std_var):\n",
    "    # To calculate the dot product of X and weights\n",
    "    dot_prod = np.dot(X, weight)\n",
    "    # To add the bias in the dot product\n",
    "    dot_prod =  dot_prod + w0\n",
    "    #to get shape of above dot product\n",
    "    shape_dot_prod = dot_prod.shape\n",
    "    # adding noise to the dot product\n",
    "    noise = noise_def(std_var,shape_dot_prod)\n",
    "    t = dot_prod + noise\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f15c542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit testing\n",
    "# x = data_matrix(100,5)\n",
    "# w = np.random.normal(5)\n",
    "# w0 = 5\n",
    "# std_dev = 0.1\n",
    "# t = depend_t(x,w,w0,std_dev)\n",
    "# print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2feabf73",
   "metadata": {},
   "source": [
    "### 3. Write a function to compute a linear regression estimate.\n",
    "\n",
    "a) Input: data matrix X and weight vector w\n",
    "\n",
    "b) Output: y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e894a0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to compute a linear regression estimate\n",
    "def linear_reg_estim(X, w):# here x is data matrix and w is weight vector\n",
    "    # matrix multiplication using \"@\" operator\n",
    "    y = ((np.dot(X, w)) + w0)\n",
    "    return y\n",
    "# def linear_reg_estim(X, w):\n",
    "#     # Add column of ones to X for bias term\n",
    "#     X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "#     # Compute the linear regression estimate\n",
    "#     y = np.dot(X, np.hstack((w0,w)))\n",
    "#     return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5124fd7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit testing\n",
    "# X = data_matrix(100,5)\n",
    "# w = np.random.randn(5)\n",
    "# w0 = 5\n",
    "# y = linear_reg_estim(X,w)\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf344abf",
   "metadata": {},
   "source": [
    "### 4.Write a function to compute the mean square error of two vectors y and t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "edd7071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to compute the mean square error\n",
    "def MSE(y,t):\n",
    "    error = np.sum(np.square(y,t)) / (np.shape(y)[0]) #MSE to calculation formula\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d20b8936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # unit testing\n",
    "# X = data_matrix(100,5)\n",
    "# w = np.random.randn(5)\n",
    "# std_dev = 0.1\n",
    "# w0 = 5\n",
    "# t = depend_t(X,w,w0,std_dev)\n",
    "# y = linear_reg_estim(X,w)\n",
    "# mse = MSE(y,t)\n",
    "# mse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889f6beb",
   "metadata": {},
   "source": [
    "### 5. Write a function to estimate the weights of linear regression using pseudo-inverse, assuming L2 regularization\n",
    "a) Input: X, t, and lambda\n",
    "\n",
    "b) Output: w, MSE, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8ae964ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input to the function is a matrix X of feature values, a vector t of target values, and a scalar\n",
    "# lamda representing the regularization strength. The output is a vector w of weights,\n",
    "# a scalar MSE representing the mean squared error between the predicted and actual target values,\n",
    "# and a vector y of predicted target values.\n",
    "def linear_regression_pinv(X, t, lamda):\n",
    "    \n",
    "    # Add a column of ones to the input matrix X for the bias term\n",
    "    # X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "    I = np.identity(X.shape[1]) #np.identity for the identity matrix\n",
    "    # Calculate the weights\n",
    "    #this function uses the numpy library and its function np.linalg.pinv for the pseudo-inverse calculation\n",
    "    w = np.linalg.pinv(X.T @ X + lamda * I) @ X.T @ t #where (lamda * I) is the regulaization term\n",
    "    y = linear_reg_estim(X,w)\n",
    "    loss_Mse = MSE(y,t)# a scalar MSE representing the mean squared error between the predicted and actual target values,\n",
    "    return w,y,loss_Mse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1169887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit testing\n",
    "# X = data_matrix(100,5)\n",
    "# w = np.random.randn(5)\n",
    "# std_dev = 0.1\n",
    "# w0 = 5\n",
    "# t = depend_t(X,w,w0,std_dev)\n",
    "# w,y,loss_Mse = linear_regression_pinv(X,t,0.2)\n",
    "# print(w,y,loss_Mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0651ad49",
   "metadata": {},
   "source": [
    "### 6. Write a function to compute the gradient of MSE with respect to its weight vector.\n",
    "a) Input: X matrix, t vector, and w vector\n",
    "\n",
    "b) Output: gradient vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23dcdddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input to the function is a matrix X of feature values,\n",
    "# a vector t of target values, and a vector w of weights. \n",
    "# The output is a gradient vector.\n",
    "# This function calculates the dot product of X and w (y)\n",
    "# which is the prediction, then computes the gradient of the MSE \n",
    "# by taking the derivative of the MSE function with respect to w.\n",
    "# The gradient is the sum of the product of the transpose of X\n",
    "# and the difference between y and t, all divided by the number of samples\n",
    "def mse_grad(X, t, w):\n",
    "    y = linear_reg_estim(X,w)\n",
    "    N = np.shape(X)[0] # to calculate number of samples\n",
    "    grad = (2*X.T) @ (y - t) / N #applying gradient MSE formula\n",
    "    return grad\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b222c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit testing\n",
    "# X = data_matrix(100,5)\n",
    "# w = np.random.randn(5)\n",
    "# std_dev = 0.1\n",
    "# w0 = 5\n",
    "# t = depend_t(X,w,w0,std_dev)\n",
    "# grad = mse_grad(X,t,w)\n",
    "# print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15587c03",
   "metadata": {},
   "source": [
    "### 7. Write a function to compute L2 norm of a vector w passed as a numpy array. Exclude bias w0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76100ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function to compute L2 norm\n",
    "def L2_norm(w):\n",
    "    # exclude bias term (w0)\n",
    "    w = w[1:]\n",
    "    # calculate L2 norm\n",
    "    norm = np.linalg.norm(w)\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00bf18ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit testing\n",
    "# w = np.random.randn(5)\n",
    "# w0 = 5\n",
    "# w[0] = w0\n",
    "# norm = L2_norm(w)\n",
    "# print(norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731f08ab",
   "metadata": {},
   "source": [
    "### 8. Write a function to compute the gradient of L2 norm with respect to the weight vectors.\n",
    "\n",
    "a) Input: X matrix and w vector\n",
    "\n",
    "b) Output: gradient vector, where gradient with respect to w0 is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5444b828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function takes in the input matrix X and weight vector w,\n",
    "# calculates the L2 norm of the weight vector, and \n",
    "# then calculates the gradient vector.\n",
    "# The gradient with respect to the first element of the weight vector (w0) is set to 0.\n",
    "def L2_norm_gradient(X, w):\n",
    "    # calculate the L2 norm of the weight vector\n",
    "    L2norm = L2_norm(w)\n",
    "    # calculate the gradient vector\n",
    "    grad = 2 * w / L2norm\n",
    "    # set the gradient with respect to w0 to 0\n",
    "    grad[0] = 0\n",
    "    return grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "869b555a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit testing\n",
    "# X = data_matrix(100,5)\n",
    "# w = np.random.randn(5)\n",
    "# w0 = 5\n",
    "# w[0] = w0\n",
    "# grad = L2_norm_gradient(X,w)\n",
    "# print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62193de7",
   "metadata": {},
   "source": [
    "### 9. Write a function to compute L1 norm of a vector w passed as a numpy array. Exclude bias w0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "14522def",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function uses numpy's abs function to take the absolute value of each element\n",
    "# in the array, and then uses the sum function to add up all of the absolute values.\n",
    "# The slice w[1:] is to exclude the bias term w0.\n",
    "#define a function to compute L1 norm\n",
    "def L1_norm(w):\n",
    "    norm = np.abs(w[1:]).sum()\n",
    "    return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "114d517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit testing\n",
    "# w = np.random.randn(5)\n",
    "# w0 = 5\n",
    "# w[0] = w0\n",
    "# norm = L1_norm(w)\n",
    "# print(norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93731715",
   "metadata": {},
   "source": [
    "### 10. Write a function to compute the gradient of L1 norm with respect to the weight vectors.\n",
    "\n",
    "a) Input: X matrix and w vector\n",
    "\n",
    "b) Output: gradient vector, where gradient with respect to w0 is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d03b01e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function takes in the input matrix X and weight vector w,\n",
    "# calculates the L2 norm of the weight vector, and \n",
    "# then calculates the gradient vector.\n",
    "# The gradient with respect to the first element of the weight vector (w0) is set to 0.\n",
    "def L1_norm_gradient(X, w):\n",
    "    # calculate the L1 norm of the weight vector\n",
    "    L1norm = L1_norm(w)\n",
    "    # calculate the gradient vector\n",
    "    grad = 2 * w / L1norm\n",
    "    # set the gradient with respect to w0 to 0\n",
    "    grad[0] = 0\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edf67660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit testing\n",
    "# X = data_matrix(100,5)\n",
    "# w = np.random.randn(5)\n",
    "# w0 = 5\n",
    "# w[0] = w0\n",
    "# grad = L1_norm_gradient(X,w)\n",
    "# print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d559a4",
   "metadata": {},
   "source": [
    "### 11. Write a function for a single update of weights of linear regression using gradient descent.\n",
    "\n",
    "a) Input: X, t, w, eta, lambda 2, lambda 1. Note that the weight of MSE will be 1\n",
    "\n",
    "b) Output: updated weight and updated MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "657efd16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes input X, t, w, eta, lambda 2, lambda 1 and \n",
    "# will update the weights using gradient descent with L1 and L2 regularization.\n",
    "# It will output the updated weight and the mean squared error.\n",
    "# It uses the equation : w = w - eta*(X.T @ error + lambda_2w + lambda_1np.sign(w))\n",
    "# eta = 0.2\n",
    "# lambda_1=L1_norm(w)\n",
    "# lambda_2=L2_norm(w)\n",
    "\n",
    "# define a function for a single update of weights of linear regression using gradient descent.\n",
    "def linear_regression_update(X, t, w, eta, lambda_2, lambda_1):\n",
    "    y = linear_reg_estim(X,w) #calculating y by calling linear regression estimation  function\n",
    "    gradient = 2 * np.dot(X.T, (y - t)) / X.shape[0] + 2 * lambda_2 * w + lambda_1 * np.sign(w)\n",
    "    #gradient = X.T @ error + lambda_2*w + lambda_1*np.sign(w)\n",
    "    w = w - eta*gradient\n",
    "    y = linear_reg_estim(X,w) # recalculating y from new weight\n",
    "    t = depend_t(X,w,w0,eta) #calling depend_t function to calculate target variable\n",
    "    mse = MSE(y,t) #calculating MSE by calling MSE function\n",
    "    return w, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c556c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit testing\n",
    "# X = data_matrix(100,5)\n",
    "# w = np.random.randn(5)\n",
    "# eta = 0.2\n",
    "# lambda_1=L1_norm(w)\n",
    "# lambda_2=L2_norm(w)\n",
    "# std_dev = 0.1\n",
    "# w0 = 5\n",
    "# t = depend_t(X,w,w0,std_dev)\n",
    "# w,mse = linear_regression_update(X, t, w, eta, lambda_2, lambda_1)\n",
    "# print(w,mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c339838a",
   "metadata": {},
   "source": [
    "### 12. Write a function to estimate the weights of linear regression using gradient descent.\n",
    "\n",
    "a) Inputs: X, t, lambda2 (default 0), lambda1 (default 0), eta, max_iter, min_change_NRMSE\n",
    "\n",
    "b) Output: Final w, final RMSE normalized with respect to variance of t.\n",
    "\n",
    "c) Stopping criteria: Either max_iter has been reached, or the normalized RMSE does not change by more than min_change_NRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbecb5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes the input X and t (target values) and the hyperparameters lambda2, lambda1, eta (learning rate),\n",
    "# max_iter (maximum number of iterations), and min_change_NRMSE (minimum change in normalized root mean squared error).\n",
    "# It returns the final weight vector w and the final normalized root mean squared error.\n",
    "# The stopping criteria are either reaching the maximum number of iterations \n",
    "# or the change in normalized root mean squared error is less than the minimum change.\n",
    "\n",
    "#define a a function to estimate the weights of linear regression using gradient descent\n",
    "def linear_regression_weights_gd(X, t, lambda2, lambda1, eta, max_iter, min_change_NRMSE):\n",
    "    w = np.zeros(X.shape[1])\n",
    "    t_var = np.var(t) #using numpy to calculate variance of t\n",
    "    RMSE_prev = float(\"inf\")\n",
    "    for i in range(max_iter):\n",
    "        y = linear_reg_estim(X,w)\n",
    "        RMSE = MSE(y,t) / (t_var + 1e-8)\n",
    "        if abs(RMSE - RMSE_prev) < min_change_NRMSE:\n",
    "            break\n",
    "        RMSE_prev = RMSE\n",
    "        w1, RMSE = linear_regression_update(X,t,w,eta,lambda2,lambda1) \n",
    "    return w1, RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ee153d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#unit testing\n",
    "# std_dev = 0.1\n",
    "# X = data_matrix(100,5)\n",
    "# w0 = 2\n",
    "# w = np.random.randn(5)\n",
    "# t = depend_t(X,w,w0,std_dev)\n",
    "# w1,RMSE = linear_regression_weights_gd(X,t,0,0,0.1,100,0.0001)\n",
    "# print(w1,RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abed9937",
   "metadata": {},
   "source": [
    "### 13. Run multiple experiments (with different random seeds) for, plot the results of (box plots), and comment on the trends and potential reasons for the following relations:\n",
    "\n",
    "#### a) Training and validation NRMSE obtained using pseudo inverse with number of training samples.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8f320f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate NRMSE\n",
    "def NRMSE(t_train,t_pred):\n",
    "    return np.sqrt(np.mean((t_train - t_pred)**2)) / np.std(t_train) #using formula for calculating NRMSE\n",
    "\n",
    "# define a function to split into training and validation data\n",
    "def split(x,t,train_frac):\n",
    "    N = X.shape[0] #sample size\n",
    "    N_train= int(N*train_frac) \n",
    "    N_val = N - N_train\n",
    "    indices = np.arange(N)\n",
    "    # Split data into training and validation sets\n",
    "    indices = np.random .permutation(indices)\n",
    "    X_train = X[indices[:N_train]]\n",
    "    t_train = t[indices[:N_train]]\n",
    "    X_val = X[indices[N_train:]]\n",
    "    t_val = t[indices[N_train:]]\n",
    "    return X_train,t_train,X_val,t_val\n",
    "\n",
    "#define a function to calculate Training and validation NRMSE obtained using pseudo inverse with number of training samples.\n",
    "def experiment_Pseudo(X,t,lamda,training_samples,train_frac):\n",
    "    # Initialize arrays to store NRMSE results for each experiment\n",
    "    training_nrmse = np.zeros(training_samples)\n",
    "    validation_nrmse = np.zeros(training_samples)\n",
    "    for i in range(training_samples):\n",
    "        X_train , t_train , X_val , t_val = split(X,t,train_frac)\n",
    "        w1,_,_ = linear_regression_pinv(X_train,t_train,lamda)\n",
    "        w2,_,_ = linear_regression_pinv(X_val,t_val,lamda)\n",
    "        # Calculate NRMSE for training and validation data\n",
    "        t_pred_train =linear_reg_estim(X_train , w1)\n",
    "        t_pred_val =linear_reg_estim(X_val , w2)\n",
    "        # Store NRMSE results\n",
    "        training_nrmse[i] = NRMSE(t_train,t_pred_train)\n",
    "        validation_nrmse[i] = NRMSE(t_val,t_pred_val)\n",
    "    return training_nrmse,validation_nrmse  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f7bb1f79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initializing terms\n",
    "std_dev = 0.1\n",
    "np.random.seed(1)\n",
    "N = 100\n",
    "D = 5\n",
    "X = data_matrix(N,D)\n",
    "w0 = 2\n",
    "w = np.random.randn(D)\n",
    "t = depend_t(X,w,w0,std_dev)\n",
    "n_experiments = 10\n",
    "lamda = 0.01\n",
    "train_frac = 0.2\n",
    "start_trials = 20\n",
    "#define a empty list\n",
    "val = []\n",
    "#define x axis label for plot\n",
    "label= [\"{}\".format(x*start_trials)for x in range(1,n_experiments+1)]\n",
    "\n",
    "#running a for loop to get different training validation nrmse for different number of variables\n",
    "for training_samples in range(10,110,10):\n",
    "    #calling experiment psedo function to get training and validation samples\n",
    "    training_nrmse, validation_nrmse = experiment_Pseudo(X,t,lamda,training_samples,train_frac)\n",
    "    val.append(validation_nrmse)#adding validation nrmse in the list\n",
    "#ploting boxplot   \n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(val)\n",
    "ax.set_xticklabels(label)\n",
    "ax.set_ylabel('NRMSE')\n",
    "ax.set_xlabel('Number of samples')\n",
    "plt.title(\"validation NRMSE obtained using pseudo inverse with number of training samples\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# we can absorb from the plot that as we increase the number of samples the median of the Nrmse become more stable\n",
    "# and nrmse decrease a little bit because as we increse number of samples the model will train and set the validation\n",
    "# data in better way "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597d87e9",
   "metadata": {},
   "source": [
    "#### b) Training and validation NRMSE obtained using pseudo inverse with number of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e06fddec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#initializing terms\n",
    "np.random.seed(1)\n",
    "N = 100\n",
    "D = 5\n",
    "std_dev = 0.1\n",
    "X = data_matrix(N,D)\n",
    "w0 = 5\n",
    "w = np.random.randn(D)\n",
    "t = depend_t(X,w,w0,std_dev)\n",
    "lamda = 0.01\n",
    "train_frac = 0.2\n",
    "num_trails = 20\n",
    "#define a empty list\n",
    "val =[]\n",
    "#define x axis label for plot\n",
    "label= [\"vari:{}\".format(X) for X in range(10,51,10)]\n",
    "\n",
    "#running a for loop to get different training validation nrmse for different number of variables\n",
    "for num_var in range(10,51,10):\n",
    "    training_nrmse, validation_nrmse = experiment_Pseudo(X[:,:num_var],t,lamda,num_trails,train_frac)\n",
    "    val.append(validation_nrmse)#adding validation nrmse in the list\n",
    "#ploting boxplot  \n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(val)\n",
    "ax.set_xticklabels(label)\n",
    "ax.set_ylabel('NRMSE')\n",
    "ax.set_xlabel('Number of variables')\n",
    "plt.title(\"validation NRMSE obtained using pseudo inverse with number of variables\")\n",
    "plt.show()\n",
    "# we can absorb from the plot that as we increase the number of variables there is an almost constant behaviour in \n",
    "# the plot because  as we change  the number of variables there is no much effect in nrmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd17ee6",
   "metadata": {},
   "source": [
    "#### c) Training and validation NRMSE obtained using pseudo inverse with noise variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "aac80ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#initializing terms\n",
    "np.random.seed(1)\n",
    "N = 100\n",
    "D = 5\n",
    "std_dev = 0.1\n",
    "X = data_matrix(N,D)\n",
    "w0 = 5\n",
    "w = np.random.randn(D)\n",
    "#define a empty list\n",
    "val = []\n",
    "#define x axis label for plot\n",
    "label = [\"vars:{}\".format(X/10) for X in range(1,6,1)]\n",
    "\n",
    "#running a for loop to get different training validation nrmse for different noise variance\n",
    "for var in range(1,6,1):\n",
    "    t = depend_t(X,w,w0,float(var/10))\n",
    "    training_nrmse, validation_nrmse = experiment_Pseudo(X,t,lamda,num_trails,train_frac)\n",
    "    val.append(validation_nrmse)#adding validation nrmse in the list\n",
    "#ploting boxplot  \n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(val)\n",
    "ax.set_xticklabels(label)\n",
    "ax.set_ylabel('NRMSE')\n",
    "ax.set_xlabel('noise variance')\n",
    "plt.title(\"validation NRMSE obtained using pseudo inverse with noise varinace\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c8b1ee",
   "metadata": {},
   "source": [
    "#### d)Training and validation NRMSE obtained using pseudo inverse with w0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a39357",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "N = 100\n",
    "D = 5\n",
    "std_dev = 0.1\n",
    "X = data_matrix(N,D)\n",
    "w0 = 5\n",
    "w = np.random.randn(D)\n",
    "val = []\n",
    "label = [\"trails:{}\".format(X) for X in range(1,11,2)]\n",
    "\n",
    "for var in range(1,11,2):\n",
    "    w[0] = w0 \n",
    "    training_nrmse, validation_nrmse = experiment_Pseudo(X,t,lamda,num_trails,train_frac)\n",
    "    val.append(validation_nrmse)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(val)\n",
    "ax.set_xticklabels(label)\n",
    "ax.set_ylabel('NRMSE')\n",
    "plt.title(\"validation NRMSE obtained using pseudo inverse with w0.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6d5fd3",
   "metadata": {},
   "source": [
    "#### e) Training and validation NRMSE obtained using pseudo inverse with lambda2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6adc315",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "N = 100\n",
    "D = 5\n",
    "w0 = 5\n",
    "std_dev = 0.1\n",
    "lamda2 = []\n",
    "for i in range(1,6):\n",
    "    np.random.seed(i)\n",
    "    X = data_matrix(N,D)\n",
    "    w = np.random.randn(D)\n",
    "    lamda2.append(L2_norm(w))\n",
    "    \n",
    "val = []\n",
    "label = [\"lamda2_{}\".format(X) for X in range(1,6)]\n",
    "\n",
    "for var_L2 in lamda2: \n",
    "    training_nrmse, validation_nrmse_lamda2 = experiment_Pseudo(X,t,var_L2,num_trails,train_frac)\n",
    "    val.append(validation_nrmse_lamda2)\n",
    "\n",
    "optimal_lamda2 = lamda2[np.argmin(validation_nrmse_lamda2)]\n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(val)\n",
    "ax.set_xticklabels(label)\n",
    "ax.set_ylabel('NRMSE')\n",
    "plt.title(\"NRMSE obtained using pseudo inverse with lambda2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3ed145",
   "metadata": {},
   "source": [
    "#### f) Time taken to solve pseudo inverse with number of samples and number of variables and its breaking points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e8e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_inv(X):\n",
    "    I = np.identity(X.shape[1]) #np.identity for the identity matrix\n",
    "    # Calculate the weights\n",
    "    #this function uses the numpy library and its function np.linalg.pinv for the pseudo-inverse calculation\n",
    "    p = np.linalg.pinv(X.T @ X) @ X.T\n",
    "    return p\n",
    "\n",
    "samples = np.arange(10,1000,10)\n",
    "variable = np.arange(10,1000,10)\n",
    "\n",
    "time_taken = []\n",
    "\n",
    "for i in range(len(samples)):\n",
    "    X = data_matrix(samples[i],variable[i])\n",
    "    #to get starting time\n",
    "    start_t = time.time()    \n",
    "    #calling pseudo inverse function to calculate pseudo inverse of different samples\n",
    "    pseudo_inverse = pseudo_inv(X)\n",
    "    #to get end time \n",
    "    end_t = time.time()\n",
    "    #calculate time taken for one iteration and append in the list\n",
    "    timetaken = (end_t-start_t)\n",
    "    time_taken.append(timetaken)\n",
    "\n",
    "plt.scatter(samples,variable,c=time_taken,cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.ylabel('number of variables')\n",
    "plt.xlabel('number of samples')\n",
    "plt.show()\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da273d5",
   "metadata": {},
   "source": [
    "#### g) Training and validation NRMSE obtained using gradient descent with max_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1f61fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_gradient_max_iter(max_iter,X,t,lamda,training_samples,train_frac):\n",
    "    # Initialize arrays to store NRMSE results for each experiment\n",
    "    training_nrmse = np.zeros(training_samples)\n",
    "    validation_nrmse = np.zeros(training_samples)\n",
    "    for i in range(training_samples):\n",
    "        X_train , t_train , X_val , t_val = split(X,t,train_frac)\n",
    "        w,_ = linear_regression_weights_gd(X_train,t_train,lamda2,lamda1,eta,max_iter,min_change_NRMSE)\n",
    "        # w = np.linalg.pinv(X_train).dot(t_train)\n",
    "        # Calculate NRMSE for traini,wng and validation data\n",
    "        t_pred_train =linear_reg_estim(X_train,w)\n",
    "        t_pred_val =linear_reg_estim(X_val,w)\n",
    "        # Store NRMSE results\n",
    "        training_nrmse[i] = NRMSE(t_train,t_pred_train)\n",
    "        validation_nrmse[i] = NRMSE(t_val,t_pred_val)\n",
    "    return training_nrmse,validation_nrmse \n",
    "\n",
    "np.random.seed(1)\n",
    "N = 100\n",
    "D = 5\n",
    "std_dev = 0.1\n",
    "X = data_matrix(N,D)\n",
    "w0 = 5\n",
    "w = np.random.randn(D)\n",
    "lamda2=0\n",
    "lamda1=0\n",
    "eta = 0.2\n",
    "max_iter = 10\n",
    "min_change_NRMSE = 0.0001\n",
    "\n",
    "val = []\n",
    "\n",
    "label = [\"max_iter:{}\".format(X) for X in range(10,210,20)]\n",
    "\n",
    "for max_iter in range(10,210,20):\n",
    "    training_nrmse, validation_nrmse = experiment_gradient_max_iter(max_iter,X,t,lamda,num_trails,train_frac)\n",
    "    val.append(validation_nrmse)\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax.boxplot(val)\n",
    "ax.set_xticklabels(label)\n",
    "ax.set_ylabel('NRMSE')\n",
    "plt.title(\"validation NRMSE obtained using gradient descent with max_iter\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dc76d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4e545c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e810cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6461e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb607620",
   "metadata": {},
   "source": [
    "#### h) Training and validation NRMSE obtained using gradient descent with eta "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec208bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_gradient_eta(eta,X,t,lamda,training_samples,train_frac):\n",
    "    # Initialize arrays to store NRMSE results for each experiment\n",
    "    training_nrmse = np.zeros(training_samples)\n",
    "    validation_nrmse = np.zeros(training_samples)\n",
    "    for i in range(training_samples):\n",
    "        X_train , t_train , X_val , t_val = split(X,t,train_frac)\n",
    "        w,_ = linear_regression_weights_gd(X_train,t_train,lamda2,lamda1,eta,max_iter,min_change_NRMSE)\n",
    "        # w = np.linalg.pinv(X_train).dot(t_train)\n",
    "        # Calculate NRMSE for traini,wng and validation data\n",
    "        t_pred_train =linear_reg_estim(X_train,w)\n",
    "        t_pred_val =linear_reg_estim(X_val,w)\n",
    "        # Store NRMSE results\n",
    "        training_nrmse[i] = NRMSE(t_train,t_pred_train)\n",
    "        validation_nrmse[i] = NRMSE(t_val,t_pred_val)\n",
    "    return training_nrmse,validation_nrmse \n",
    "\n",
    "np.random.seed(1)\n",
    "N = 100\n",
    "D = 5\n",
    "std_dev = 0.1\n",
    "X = data_matrix(N,D)\n",
    "w0 = 5\n",
    "w = np.random.randn(D)\n",
    "lamda2=0\n",
    "lamda1=0\n",
    "max_iter = 10\n",
    "min_change_NRMSE = 0.0001\n",
    "\n",
    "val = []\n",
    "\n",
    "label = [\"eta_{}\".format(X/100) for X in range(10,210,20)]\n",
    "\n",
    "for eta in range(10,210,20):\n",
    "    training_nrmse, validation_nrmse = experiment_gradient_eta(eta/100,X,t,lamda,num_trails,train_frac)\n",
    "    val.append(validation_nrmse)\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax.boxplot(val)\n",
    "ax.set_xticklabels(label)\n",
    "ax.set_ylabel('NRMSE')\n",
    "plt.title(\"validation NRMSE obtained using gradient descent with max_iter\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705b05b1",
   "metadata": {},
   "source": [
    "#### i) Time taken to solve gradient descent with number of samples and number of variables and its breaking points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1605eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 50\n",
    "min_change_NRMSE = 0.001\n",
    "eta = 0.1\n",
    "lambda_2=1.2\n",
    "lambda_1=1.5\n",
    "\n",
    "# X = data_matrix()\n",
    "def gradient_descent(X, t, lambda2, lambda1, eta, max_iter, min_change_NRMSE):\n",
    "    t_var = np.var(X)\n",
    "    RMSE_prev = float(\"inf\")\n",
    "    for i in range(max_iter):\n",
    "        y = X@w\n",
    "        RMSE = MSE(y,t) / (t_var + 1e-8)\n",
    "        if abs(RMSE - RMSE_prev) < min_change_NRMSE:\n",
    "            break\n",
    "        RMSE_prev = RMSE\n",
    "#         y = X@w\n",
    "        gradient = 2 * np.dot(X.T, (y - t)) / X.shape[0] + 2 * lambda_2 * w + lambda_1 * np.sign(w)\n",
    "    return gradient\n",
    "\n",
    "samples = np.arange(10,1000,20)\n",
    "variable = np.arange(10,1000,20)\n",
    "\n",
    "time_taken = []\n",
    "\n",
    "for i in range(len(samples)):\n",
    "    X = data_matrix(samples[i],variable[i])\n",
    "    w =np.random.randn(variable[i])\n",
    "    t = depend_t(X, w, 5, 0.6)\n",
    "    #to get starting time\n",
    "    start_t = time.time()    \n",
    "    #calling pseudo inverse function to calculate pseudo inverse of different samples\n",
    "    pseudo_inverse = gradient_descent(X,t,0,0,0.1,max_iter,min_change_NRMSE)\n",
    "    #to get end time \n",
    "    end_t = time.time()\n",
    "    #calculate time taken for one iteration and append in the list\n",
    "    timetaken = (end_t-start_t)\n",
    "    time_taken.append(timetaken)\n",
    "\n",
    "plt.scatter(samples,variable,c=time_taken,cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.ylabel('number of variables')\n",
    "plt.xlabel('number of samples')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b601fa",
   "metadata": {},
   "source": [
    "#### j) Time taken to solve gradient descent with number of variables and its breaking point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e566574",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 50\n",
    "min_change_NRMSE = 0.001\n",
    "eta = 0.1\n",
    "lambda_2=1.2\n",
    "lambda_1=1.5\n",
    "bais = 5\n",
    "var = 0.1\n",
    "N = 100\n",
    "\n",
    "# X = data_matrix()\n",
    "def gradient_descent(X, t, lambda2, lambda1, eta, max_iter, min_change_NRMSE):\n",
    "    t_var = np.var(X)\n",
    "    RMSE_prev = float(\"inf\")\n",
    "    for i in range(max_iter):\n",
    "        y = X@w\n",
    "        RMSE = MSE(y,t) / (t_var + 1e-8)\n",
    "        if abs(RMSE - RMSE_prev) < min_change_NRMSE:\n",
    "            break\n",
    "        RMSE_prev = RMSE\n",
    "#         y = X@w\n",
    "        gradient = 2 * np.dot(X.T, (y - t)) / X.shape[0] + 2 * lambda_2 * w + lambda_1 * np.sign(w)\n",
    "    return gradient\n",
    "samples = np.arange(10,1000,20)\n",
    "variable = np.arange(10,1000,20)\n",
    "\n",
    "time_taken = []\n",
    "\n",
    "for i in range(len(samples)):\n",
    "    X = data_matrix(N,variable[i])\n",
    "    w =np.random.randn(variable[i])\n",
    "    t = depend_t(X, w, bais, var)\n",
    "    #to get starting time\n",
    "    start_t = time.time()    \n",
    "    #calling pseudo inverse function to calculate pseudo inverse of different samples\n",
    "    pseudo_inverse = gradient_descent(X,t,lambda_2,lambda_1,eta,max_iter,min_change_NRMSE)\n",
    "    #to get end time \n",
    "    end_t = time.time()\n",
    "    #calculate time taken for one iteration and append in the list\n",
    "    timetaken = (end_t-start_t)\n",
    "    time_taken.append(timetaken)\n",
    "\n",
    "plt.scatter(samples,variable,c=time_taken,cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.ylabel('number of variables')\n",
    "plt.xlabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03dc7ac",
   "metadata": {},
   "source": [
    "#### k) Training and validation NRMSE and number of nearly zero weights obtained using gradient descent with lambda2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752768f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nearly_zero(w, epsilon=1e-3):\n",
    "    if np.sum(np.abs(w) < epsilon):\n",
    "        return w\n",
    "    \n",
    "def linear_regression_update(X, t, w, eta, lambda_2, lambda_1):\n",
    "    w = np.zeros(X.shape[1])\n",
    "    y = linear_reg_estim(X,w)\n",
    "    gradient = 2 * np.dot(X.T, (y - t)) / X.shape[0] + 2 * lambda_2 * w + lambda_1 * np.sign(w)\n",
    "    w = w - eta*gradient\n",
    "    y = linear_reg_estim(X,w)\n",
    "    t = depend_t(X,w,w0,eta)\n",
    "    mse = MSE(y,t)\n",
    "    return w, mse\n",
    "\n",
    "w = np.zeros(X.shape[1])\n",
    "def linear_regression_nearly_zero_weights_gd_lamda2(X, t, lamda2, lamda1, eta, max_iter, min_change_NRMSE):\n",
    "    t_var = np.var(t)\n",
    "    RMSE_prev = float(\"inf\")\n",
    "    for i in range(max_iter):\n",
    "        y = linear_reg_estim(X,w)\n",
    "        RMSE = MSE(y,t) / (t_var + 1e-8)\n",
    "        if abs(RMSE - RMSE_prev) < min_change_NRMSE:\n",
    "            break\n",
    "        RMSE_prev = RMSE\n",
    "        w2, RMSE = linear_regression_update(X,t,w,eta,lamda2,lamda1)\n",
    "        nzw = calculate_nearly_zero(w2)\n",
    "    return nzw, RMSE\n",
    "\n",
    "# def calculate_nearly_zero(w, epsilon=1e-2):\n",
    "#     return np.sum(np.abs(w) < epsilon)\n",
    "\n",
    "\n",
    "def experiment_gradient_lamda2(X,t,lamda2,eta,training_samples,train_frac):\n",
    "    # Initialize arrays to store NRMSE results for each experiment\n",
    "    training_nrmse = np.zeros(training_samples)\n",
    "    validation_nrmse = np.zeros(training_samples)\n",
    "    for i in range(training_samples):\n",
    "        X_train , t_train , X_val , t_val = split(X,t,train_frac)\n",
    "        \n",
    "#         w,_ = linear_regression_nearly_zero_weights_gd_lamda2(X_train,t_train,lamda2,0,eta,max_iter,min_change_NRMSE)\n",
    "        w = np.zeros(X.shape[1])\n",
    "        # w = np.linalg.pinv(X_train).dot(t_train)\n",
    "        # Calculate NRMSE for traini,wng and validation data\n",
    "        t_pred_train =linear_reg_estim(X_train,w)\n",
    "        t_pred_val =linear_reg_estim(X_val,w)\n",
    "        # Store NRMSE results\n",
    "        training_nrmse[i] = NRMSE(t_train,t_pred_train)\n",
    "        validation_nrmse[i] = NRMSE(t_val,t_pred_val)\n",
    "    return training_nrmse,validation_nrmse \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be72d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "N = 100\n",
    "D = 5\n",
    "std_dev = 0.1\n",
    "eta = 0.1\n",
    "# X = data_matrix(N,D)\n",
    "w0 = 5\n",
    "# w = np.random.randn(D)\n",
    "# lamda1=0\n",
    "max_iter = 100\n",
    "min_change_NRMSE = 0.0001\n",
    "std_dev = 0.1\n",
    "num_trails = 20\n",
    "lamda_2 = []\n",
    "for i in range(1,6):\n",
    "    np.random.seed(i)\n",
    "    X = data_matrix(N,D)\n",
    "    w = np.random.randn(D)\n",
    "    lamda_2.append(L2_norm(w))\n",
    "print(lamda_2)   \n",
    "val = []\n",
    "label = [\"L2_{}\".format(X) for X in range(1,6)]\n",
    "\n",
    "\n",
    "for var_L2 in lamda_2:\n",
    "    training_nrmse, validation_nrmse = experiment_gradient_lamda2(X,t,var_L2,eta,num_trails,train_frac)\n",
    "    val.append(validation_nrmse)\n",
    "#     print(validation_nrmse)\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "ax.boxplot(val)\n",
    "ax.set_xticklabels(label)\n",
    "ax.set_ylabel('NRMSE')\n",
    "plt.title(\"validation NRMSE obtained using gradient descent with max_iter\")\n",
    "plt.show()\n",
    "# w1,_ = linear_regression_nearly_zero_weights_gd_lamda2(X,t,0.1,lamda1,eta,max_iter,min_change_NRMSE)\n",
    "# w1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b0f07d",
   "metadata": {},
   "source": [
    "#### l) Training and validation NRMSE and number of nearly zero weights obtained using gradient descent with lambda1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f430d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nearly_zero(w, epsilon=1e-3):\n",
    "    if np.sum(np.abs(w) < epsilon):\n",
    "        return w\n",
    "    \n",
    "w = np.zeros(X.shape[1])\n",
    "def linear_regression_nearly_zero_weights_gd_lamda1(X, t, lamda2, lamda1, eta, max_iter, min_change_NRMSE):\n",
    "    w = np.zeros(X.shape[1])\n",
    "    t_var = np.var(t)\n",
    "    RMSE_prev = float(\"inf\")\n",
    "    for i in range(max_iter):\n",
    "        y = linear_reg_estim(X,w)\n",
    "        RMSE = MSE(y,t) / (t_var + 1e-8)\n",
    "        if abs(RMSE - RMSE_prev) < min_change_NRMSE:\n",
    "            break\n",
    "        RMSE_prev = RMSE\n",
    "        w1, RMSE = linear_regression_update(X,t,w,eta,lamda2,lamda1)\n",
    "        nzw = calculate_nearly_zero(w1)\n",
    "    return nzw, RMSE\n",
    "\n",
    "# # def calculate_nearly_zero(w, epsilon=1e-2):\n",
    "# #     return np.sum(np.abs(w) < epsilon)\n",
    "\n",
    "\n",
    "def experiment_gradient_lamda1(X,t,lamda1,eta,training_samples,train_frac):\n",
    "    # Initialize arrays to store NRMSE results for each experiment\n",
    "    training_nrmse = np.zeros(training_samples)\n",
    "    validation_nrmse = np.zeros(training_samples)\n",
    "    for i in range(training_samples):\n",
    "        X_train , t_train , X_val , t_val = split(X,t,train_frac)\n",
    "#         w,_ = linear_regression_nearly_zero_weights_gd_lamda1(X_train,t_train,0,lamda1,eta,max_iter,min_change_NRMSE)\n",
    "        w = np.zeros(X.shape[1])\n",
    "        # w = np.linalg.pinv(X_train).dot(t_train)\n",
    "        # Calculate NRMSE for traini,wng and validation data\n",
    "        t_pred_train =linear_reg_estim(X_train,w)\n",
    "        t_pred_val =linear_reg_estim(X_val,w)\n",
    "        # Store NRMSE results\n",
    "        training_nrmse[i] = NRMSE(t_train,t_pred_train)\n",
    "        validation_nrmse[i] = NRMSE(t_val,t_pred_val)\n",
    "#         print(validation_nrmse)\n",
    "    return training_nrmse,validation_nrmse \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8641e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "N = 100\n",
    "D = 5\n",
    "std_dev = 0.1\n",
    "eta = 0.1\n",
    "X = data_matrix(N,D)\n",
    "w0 = 5\n",
    "w = np.random.randn(D)\n",
    "# lamda2=0\n",
    "max_iter = 100\n",
    "min_change_NRMSE = 0.0001\n",
    "std_dev = 0.1\n",
    "num_trails = 20\n",
    "lamda_1 = []\n",
    "for i in range(1,6):\n",
    "    np.random.seed(i)\n",
    "    X = data_matrix(N,D)\n",
    "    w = np.random.randn(D)\n",
    "    lamda_1.append(L1_norm(w))\n",
    "    \n",
    "val = []\n",
    "label = [\"L1_{}\".format(X) for X in range(1,6)]\n",
    "# print(lamda_1)\n",
    "\n",
    "for var_L1 in lamda_1:\n",
    "    training_nrmse, validation_nrmse = experiment_gradient_lamda1(X,t,var_L1,eta,num_trails,train_frac)\n",
    "    val.append(validation_nrmse)\n",
    "#     print(validation_nrmse)\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "ax.boxplot(val)\n",
    "ax.set_xticklabels(label)\n",
    "ax.set_ylabel('NRMSE')\n",
    "plt.title(\"validation NRMSE obtained using gradient descent with max_iter\")\n",
    "plt.show()\n",
    "# w1,_ = linear_regression_nearly_zero_weights_gd_lamda2(X,t,0.1,lamda1,eta,max_iter,min_change_NRMSE)\n",
    "# w1\n",
    "# lamda_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad12478a",
   "metadata": {},
   "source": [
    "#### m) Training and validation NRMSE for optimal lambda2 with noise variance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab571fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "N = 100\n",
    "D = 5\n",
    "std_dev = 0.1\n",
    "X = data_matrix(N,D)\n",
    "w0 = 5\n",
    "w = np.random.randn(D)\n",
    "# optimal_lamda2 = lamda2[np.argmin(validation_nrmse_lamda2)]\n",
    "val = []\n",
    "\n",
    "label = [\"vars:{}\".format(X/10) for X in range(2,11,2)]\n",
    "\n",
    "for var in range(2,11,2):\n",
    "    t = depend_t(X,w,w0,float(var/10))\n",
    "    training_nrmse, validation_nrmse = experiment_Pseudo(X,t,optimal_lamda2,num_trails,train_frac)\n",
    "    val.append(validation_nrmse)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(val)\n",
    "ax.set_xticklabels(label)\n",
    "ax.set_ylabel('NRMSE')\n",
    "plt.title(\"Number of trails\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c7296e",
   "metadata": {},
   "source": [
    "#### n) Training and validation NRMSE for optimal lambda1 with noise variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f4860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "N = 100\n",
    "D = 5\n",
    "std_dev = 0.1\n",
    "X = data_matrix(N,D)\n",
    "w0 = 5\n",
    "w = np.random.randn(D)\n",
    "lamda1 = []\n",
    "for i in range(1,6):\n",
    "    np.random.seed(i)\n",
    "    X = data_matrix(N,D)\n",
    "    w = np.random.randn(D)\n",
    "    lamda1.append(L1_norm(w))\n",
    "    \n",
    "val = []\n",
    "label = [\"lamda_{}\".format(X) for X in range(1,6)]\n",
    "\n",
    "for var_L1 in lamda1: \n",
    "    training_nrmse, validation_nrmse_lamda1 = experiment_Pseudo(X,t,var_L1,num_trails,train_frac)\n",
    "    val.append(validation_nrmse_lamda1)\n",
    "\n",
    "optimal_lamda1 = lamda1[np.argmin(validation_nrmse_lamda1)]\n",
    "val_ = []\n",
    "\n",
    "label = [\"vars:{}\".format(X/10) for X in range(2,11,2)]\n",
    "\n",
    "for var in range(2,11,2):\n",
    "    t = depend_t(X,w,w0,float(var/10))\n",
    "    training_nrmse, validation_nrmse = experiment_Pseudo(X,t,optimal_lamda1,num_trails,train_frac)\n",
    "    val_.append(validation_nrmse)\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "ax.boxplot(val)\n",
    "ax.set_xticklabels(label)\n",
    "ax.set_ylabel('NRMSE')\n",
    "plt.title(\"validation NRMSE for optimal lambda1 with noise variance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9e0da5",
   "metadata": {},
   "source": [
    "#### o) Experiment (f) but, this time with number of training samples and number of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fb563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split(X,t,train_frac):\n",
    "#     N = X.shape[0]\n",
    "#     N_train= int(N*train_frac)\n",
    "#     indices = np.arange(N)\n",
    "#     # Split data into training and validation sets\n",
    "#     indices = np.random .permutation(indices)\n",
    "#     X_train = X[indices[:N_train]]\n",
    "#     t_train = t[indices[:N_train]]\n",
    "#     return X_train\n",
    "\n",
    "samples = np.arange(10,100,10)\n",
    "train_frac = np.arange(0.1,1,0.1)\n",
    "\n",
    "time_taken = []\n",
    "\n",
    "for i in range(len(samples)):\n",
    "    train_samples= int(samples[i]*train_frac[i])\n",
    "    indices = np.arange(samples[i])\n",
    "    # Split data into training and validation sets\n",
    "    indices = np.random.permutation(indices)\n",
    "    X_train = X[indices[:train_samples]]\n",
    "    #to get starting time\n",
    "    start_t = time.time()    \n",
    "    #calling pseudo inverse function to calculate pseudo inverse of different samples\n",
    "    pseudo_inverse = pseudo_inv(X_train)\n",
    "    #to get end time \n",
    "    end_t = time.time()\n",
    "    #calculate time taken for one iteration and append in the list\n",
    "    timetaken = (end_t-start_t)\n",
    "    time_taken.append(timetaken)\n",
    "\n",
    "plt.scatter(samples,train_frac,c=time_taken,cmap='viridis')\n",
    "plt.colorbar()\n",
    "plt.ylabel('training fraction')\n",
    "plt.xlabel('number of samples')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138b4161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b0db9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f4f6db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
